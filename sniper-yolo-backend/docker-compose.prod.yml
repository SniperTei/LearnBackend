version: '3.8'

services:
  # FastAPI 后端服务
  web:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        ENVIRONMENT: production
    image: sniper-yolo-backend:prod
    container_name: sniper_yolo_backend_prod
    restart: always
    ports:
      - "8000:8000"  # 生产环境使用标准 8000 端口
    depends_on:
      postgres:
        condition: service_healthy
    env_file:
      - .env.prod
    environment:
      - DATABASE_URL=postgresql+asyncpg://sniper:STRONG_PASSWORD_HERE@postgres:5432/sniper_yolo
      - ALEMBIC_DATABASE_URL=postgresql+psycopg2://sniper:STRONG_PASSWORD_HERE@postgres:5432/sniper_yolo
    networks:
      - sniper_prod_network
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8000/api/v1/health')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "5"
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 512M

  # PostgreSQL 数据库
  postgres:
    image: postgres:15-alpine
    container_name: sniper_postgres_prod
    restart: always
    environment:
      - POSTGRES_USER=sniper
      - POSTGRES_PASSWORD=STRONG_PASSWORD_HERE_CHANGE_THIS
      - POSTGRES_DB=sniper_yolo
      - POSTGRES_INITDB_ARGS=--encoding=UTF-8 --locale=C
    volumes:
      - postgres_data_prod:/var/lib/postgresql/data
      - ./scripts/init-db.sql:/docker-entrypoint-initdb.d/init-db.sql
      - ./backups/postgres:/backups  # 备份目录
    ports:
      - "5432:5432"
    networks:
      - sniper_prod_network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U sniper"]
      interval: 10s
      timeout: 5s
      retries: 5
    logging:
      driver: "json-file"
      options:
        max-size: "20m"
        max-file: "5"
    command: >
      postgres
      -c shared_buffers=256MB
      -c max_connections=200
      -c work_mem=4MB
      -c effective_cache_size=1GB
      -c maintenance_work_mem=64MB
      -c checkpoint_completion_target=0.9
      -c wal_buffers=16MB
      -c default_statistics_target=100
      -c random_page_cost=1.1

  # Redis 缓存
  redis:
    image: redis:7-alpine
    container_name: sniper_redis_prod
    restart: always
    ports:
      - "6379:6379"
    volumes:
      - redis_data_prod:/data
    networks:
      - sniper_prod_network
    command: >
      redis-server
      --appendonly yes
      --maxmemory 512mb
      --maxmemory-policy allkeys-lru
      --save 900 1
      --save 300 10
      --save 60 10000
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Nginx 反向代理
  nginx:
    image: nginx:alpine
    container_name: sniper_nginx_prod
    restart: always
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.prod.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro  # SSL 证书目录
      - ./nginx/logs/prod:/var/log/nginx
      - ./nginx/html:/usr/share/nginx/html:ro  # 静态文件
    depends_on:
      - web
    networks:
      - sniper_prod_network
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "20m"
        max-file: "5"

networks:
  sniper_prod_network:
    name: sniper_prod_network
    driver: bridge
    ipam:
      config:
        - subnet: 172.21.0.0/16

volumes:
  postgres_data_prod:
    driver: local
  redis_data_prod:
    driver: local
